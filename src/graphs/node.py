import os
import json
from datetime import datetime
from typing import Optional, Dict, Any
from jinja2 import Template
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langgraph.runtime import Runtime
from coze_coding_utils.runtime_ctx.context import Context
from coze_coding_dev_sdk import LLMClient, ASRClient, TTSClient
import requests

from graphs.state import (
    LongTermMemoryInput, LongTermMemoryOutput,
    HomeworkCheckInput, HomeworkCheckOutput,
    ActiveCareInput, ActiveCareOutput,
    SpeakingPracticeInput, SpeakingPracticeOutput,
    RealtimeConversationInput, RealtimeConversationOutput,
    VoiceSynthesisInput, VoiceSynthesisOutput,
    RouteDecisionInput,
    RealtimeCallInput, RealtimeCallOutput
)


# ============== èŠ‚ç‚¹1ï¼šé•¿æœŸè®°å¿†èŠ‚ç‚¹ï¼ˆå†…å­˜æ–¹å¼ï¼‰ ==============
def long_term_memory_node(
    state: LongTermMemoryInput,
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> LongTermMemoryOutput:
    """
    title: é•¿æœŸè®°å¿†ç®¡ç†
    desc: ä½¿ç”¨å†…å­˜æ–¹å¼ç®¡ç†å­©å­çš„å¯¹è¯å†å²å’Œå­¦ä¹ è¿›åº¦è®°å½•
    """
    ctx = runtime.context
    
    # ä½¿ç”¨å†…å­˜å­˜å‚¨ç®¡ç†å­©å­çš„å¯¹è¯å†å²å’Œå­¦ä¹ è¿›åº¦
    from graphs.memory_store import MemoryStore
    
    memory_store = MemoryStore.get_instance()
    
    if state.action_type == "load":
        # åŠ è½½æ•°æ®
        conversation_history = memory_store.get_conversation_history(state.child_id)
        learning_progress = memory_store.get_learning_progress(state.child_id)
        speaking_practice_count = memory_store.get_speaking_practice_count(state.child_id)
        
        return LongTermMemoryOutput(
            conversation_history=conversation_history,
            learning_progress=learning_progress,
            speaking_practice_count=speaking_practice_count,
            load_success=True,
            save_success=False
        )
    
    elif state.action_type == "save":
        # ä¿å­˜æ•°æ®
        if state.conversation_record:
            memory_store.add_conversation(state.child_id, state.conversation_record)
        if state.learning_progress:
            memory_store.update_learning_progress(state.child_id, state.learning_progress)
        
        return LongTermMemoryOutput(
            load_success=False,
            save_success=True
        )
    
    return LongTermMemoryOutput(
        load_success=False,
        save_success=False
    )


# ============== èŠ‚ç‚¹2ï¼šä½œä¸šæ£€æŸ¥èŠ‚ç‚¹ ==============
def homework_check_node(
    state: HomeworkCheckInput,
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> HomeworkCheckOutput:
    """
    title: ä½œä¸šæ£€æŸ¥ä¸æé†’
    desc: æ£€æŸ¥ä½œä¸šå®ŒæˆçŠ¶æ€ï¼ŒåŸºäºæ—¶é—´æœ‰æ•ˆæ€§è¿‡æ»¤è¿‡æœŸä½œä¸šï¼Œç”Ÿæˆæé†’æ¶ˆæ¯
    """
    ctx = runtime.context
    
    homework_status = "æ— ä½œä¸š"
    need_remind = False
    remind_message = ""
    
    # ä½¿ç”¨MemoryStoreè·å–æœ‰æ•ˆçš„ä½œä¸šï¼ˆè‡ªåŠ¨è¿‡æ»¤è¿‡æœŸå’Œå·²å®Œæˆçš„ä½œä¸šï¼‰
    from graphs.memory_store import MemoryStore
    memory_store = MemoryStore.get_instance()
    
    # è·å–æœ‰æ•ˆçš„ä½œä¸šåˆ—è¡¨
    valid_homework = memory_store.get_valid_homework(state.child_id)
    
    if valid_homework:
        homework_status = "æœªå¼€å§‹"
        need_remind = True
        
        # ç”Ÿæˆæé†’æ¶ˆæ¯ï¼ˆåŒ…å«æˆªæ­¢æ—¶é—´ä¿¡æ¯ï¼‰
        subjects_with_deadline = []
        for hw in valid_homework:
            subject = hw.get("subject", "æœªçŸ¥")
            deadline_str = hw.get("deadline", "")
            
            if deadline_str:
                try:
                    deadline = datetime.fromisoformat(deadline_str)
                    now = datetime.now()
                    hours_left = (deadline - now).total_seconds() / 3600
                    
                    if hours_left < 24:
                        subjects_with_deadline.append(f"{subject}ï¼ˆå‰©ä½™{int(hours_left)}å°æ—¶ï¼‰")
                    else:
                        days_left = int(hours_left / 24)
                        subjects_with_deadline.append(f"{subject}ï¼ˆå‰©ä½™{days_left}å¤©ï¼‰")
                except (ValueError, TypeError):
                    subjects_with_deadline.append(subject)
            else:
                subjects_with_deadline.append(subject)
        
        remind_message = f"å®è´ï¼Œä½ è¿˜æœ‰{len(valid_homework)}é¡¹ä½œä¸šéœ€è¦å®Œæˆå“¦ï¼š{', '.join(subjects_with_deadline)}ã€‚è¦ä¸è¦ç°åœ¨å¼€å§‹åšä½œä¸šå‘¢ï¼Ÿ"
    else:
        homework_status = "æ— ä½œä¸š"
        remind_message = "ä»Šå¤©æ²¡æœ‰éœ€è¦å®Œæˆçš„ä½œä¸šï¼ŒçœŸæ£’ï¼å¯ä»¥å°½æƒ…ç©è€å•¦ï½"
    
    return HomeworkCheckOutput(
        homework_status=homework_status,
        need_remind=need_remind,
        remind_message=remind_message
    )


# ============== èŠ‚ç‚¹3ï¼šä¸»åŠ¨å…³å¿ƒèŠ‚ç‚¹ï¼ˆAgentèŠ‚ç‚¹ï¼‰ ==============
def active_care_node(
    state: ActiveCareInput,
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> ActiveCareOutput:
    """
    title: ä¸»åŠ¨å…³å¿ƒ
    desc: åŸºäºå­©å­çš„çŠ¶æ€å’Œå†å²å¯¹è¯ï¼Œä¸»åŠ¨å‘èµ·å…³å¿ƒçš„å¯¹è¯
    integrations: å¤§è¯­è¨€æ¨¡å‹
    """
    ctx = runtime.context
    
    # è¯»å–é…ç½®æ–‡ä»¶
    cfg_file = os.path.join(os.getenv("COZE_WORKSPACE_PATH"), config['metadata']['llm_cfg'])
    with open(cfg_file, 'r') as fd:
        _cfg = json.load(fd)
    
    llm_config = _cfg.get("config", {})
    sp = _cfg.get("sp", "")
    up = _cfg.get("up", "")
    
    # ä½¿ç”¨jinja2æ¨¡æ¿æ¸²æŸ“æç¤ºè¯
    sp_tpl = Template(sp)
    up_tpl = Template(up)
    
    system_prompt = sp_tpl.render({
        "child_name": state.child_name,
        "child_age": state.child_age,
        "interests": ", ".join(state.child_interests)
    })
    
    user_prompt = up_tpl.render({
        "current_time": state.current_time,
        "conversation_history": state.conversation_history[-3:] if state.conversation_history else [],
        "interests": ", ".join(state.child_interests)
    })
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    client = LLMClient(ctx=ctx)
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    
    # è°ƒç”¨å¤§æ¨¡å‹
    response = client.invoke(
        messages=messages,
        model=llm_config.get("model", "doubao-seed-1-8-251228"),
        temperature=llm_config.get("temperature", 0.8)
    )
    
    # æå–å“åº”æ–‡æœ¬
    if isinstance(response.content, str):
        care_message = response.content
    else:
        care_message = str(response.content)
    
    return ActiveCareOutput(care_message=care_message.strip())


# ============== èŠ‚ç‚¹4ï¼šå£è¯­ç»ƒä¹ èŠ‚ç‚¹ ==============
def speaking_practice_node(
    state: SpeakingPracticeInput,
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> SpeakingPracticeOutput:
    """
    title: å£è¯­ç»ƒä¹ 
    desc: è¯­éŸ³è¯†åˆ«ã€çº æ­£å’Œåé¦ˆï¼Œå¸®åŠ©å­©å­ç»ƒä¹ å£è¯­
    integrations: è¯­éŸ³å¤§æ¨¡å‹, å¤§è¯­è¨€æ¨¡å‹
    """
    ctx = runtime.context
    
    recognized_text = state.user_input_text
    
    # å¦‚æœæœ‰éŸ³é¢‘è¾“å…¥ï¼Œè¿›è¡Œè¯­éŸ³è¯†åˆ«
    if state.user_input_audio and not recognized_text:
        asr_client = ASRClient(ctx=ctx)
        
        try:
            # ä½¿ç”¨URLè¿›è¡Œè¯­éŸ³è¯†åˆ«
            text, data = asr_client.recognize(
                uid=f"{state.child_name}_practice",
                url=state.user_input_audio.url
            )
            recognized_text = text
        except Exception as e:
            recognized_text = state.user_input_text  # é™çº§åˆ°ä½¿ç”¨æ–‡æœ¬è¾“å…¥
    
    # æ„å»ºå­¦ä¹ è¿›åº¦ä¿¡æ¯
    practice_count = 0
    from graphs.memory_store import MemoryStore
    memory_store = MemoryStore.get_instance()
    learning_progress = memory_store.get_learning_progress(state.child_name)
    
    if learning_progress and "speaking_practice_count" in learning_progress:
        practice_count = learning_progress["speaking_practice_count"]
    
    # ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¯­éŸ³çº æ­£å’Œåé¦ˆ
    client = LLMClient(ctx=ctx)
    
    feedback_prompt = f"""å­©å­çš„å§“åï¼š{state.child_name}ï¼Œå¹´é¾„ï¼š{state.child_age}å²
å­©å­åˆšæ‰è¯´ï¼š{recognized_text}
å·²ç»ƒä¹ æ¬¡æ•°ï¼š{practice_count}

è¯·å¯¹å­©å­çš„å£è¯­è¡¨è¾¾è¿›è¡Œè¯„ä»·å’ŒæŒ‡å¯¼ï¼š
1. çº æ­£å‘éŸ³æˆ–è¯­æ³•é”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
2. ç»™äºˆé¼“åŠ±å’Œè‚¯å®š
3. æä¾›æ”¹è¿›å»ºè®®
4. æå‡ºä¸€ä¸ªç›¸å…³çš„é—®é¢˜å¼•å¯¼ç»§ç»­å¯¹è¯

è¯·ç”¨å‹å¥½ã€é¼“åŠ±çš„è¯­æ°”å›å¤ï¼Œé€‚åˆ{state.child_age}å²çš„å­©å­ç†è§£ã€‚

é‡è¦æé†’ï¼š
- åªè¾“å‡ºå¯¹è¯å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•åŠ¨ä½œæè¿°ï¼ˆå¦‚ï¼šï¼ˆå¾®ç¬‘ï¼‰ã€ï¼ˆç‚¹å¤´ï¼‰ã€ï¼ˆé¼“æŒï¼‰ç­‰ï¼‰
- ä¸è¦åŒ…å«æ‹¬å·å†…çš„ä»»ä½•æ–‡å­—
- ä¸è¦ä½¿ç”¨è¡¨æƒ…ç¬¦å·
- ç›´æ¥ç”¨æ–‡å­—è¡¨è¾¾ä½ çš„é¼“åŠ±å’ŒæŒ‡å¯¼"""
    
    messages = [
        HumanMessage(content=feedback_prompt)
    ]
    
    response = client.invoke(
        messages=messages,
        model="doubao-seed-1-8-251228",
        temperature=0.8
    )
    
    if isinstance(response.content, str):
        feedback = response.content
    else:
        feedback = str(response.content)
    
    # æ›´æ–°ç»ƒä¹ æ¬¡æ•°
    new_practice_count = practice_count + 1
    memory_store.update_learning_progress(state.child_name, {
        "speaking_practice_count": new_practice_count,
        "last_practice_time": datetime.now().isoformat()
    })
    
    return SpeakingPracticeOutput(
        recognized_text=recognized_text,
        corrected_text=recognized_text,  # LLMä¼šè‡ªåŠ¨åœ¨åé¦ˆä¸­çº æ­£
        feedback=feedback.strip(),
        practice_count=new_practice_count
    )


# ============== èŠ‚ç‚¹5ï¼šå®æ—¶å¯¹è¯èŠ‚ç‚¹ï¼ˆAgentèŠ‚ç‚¹ï¼‰ ==============
def realtime_conversation_node(
    state: RealtimeConversationInput,
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> RealtimeConversationOutput:
    """
    title: å®æ—¶å¯¹è¯
    desc: ä¸å­©å­è¿›è¡Œå®æ—¶å¯¹è¯ï¼Œå›åº”å­©å­çš„é—®é¢˜å’Œéœ€æ±‚ï¼ˆæ”¯æŒæ—¶é—´æ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡ï¼‰
    integrations: å¤§è¯­è¨€æ¨¡å‹
    """
    ctx = runtime.context
    
    # è¯»å–é…ç½®æ–‡ä»¶
    cfg_file = os.path.join(os.getenv("COZE_WORKSPACE_PATH"), config['metadata']['llm_cfg'])
    with open(cfg_file, 'r') as fd:
        _cfg = json.load(fd)
    
    llm_config = _cfg.get("config", {})
    sp = _cfg.get("sp", "")
    up = _cfg.get("up", "")
    
    # æ¸²æŸ“ç³»ç»Ÿæç¤ºè¯
    sp_tpl = Template(sp)
    system_prompt = sp_tpl.render({
        "child_name": state.child_name,
        "child_age": state.child_age
    })
    
    # è·å–å½“å‰æ—¶é—´ä¿¡æ¯
    current_time = datetime.now()
    time_of_day = "æ—©ä¸Š" if current_time.hour < 12 else "ä¸‹åˆ" if current_time.hour < 18 else "æ™šä¸Š"
    current_date = current_time.strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    # æ¸²æŸ“ç”¨æˆ·æç¤ºè¯ï¼ˆåŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰
    up_tpl = Template(up)
    user_prompt = up_tpl.render({
        "user_input": state.user_input_text,
        "context_info": state.context_info,
        "conversation_history": state.conversation_history[-3:] if state.conversation_history else [],
        "current_time": current_time.strftime("%H:%M"),
        "time_of_day": time_of_day,
        "current_date": current_date
    })
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    client = LLMClient(ctx=ctx)
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    
    # è°ƒç”¨å¤§æ¨¡å‹
    response = client.invoke(
        messages=messages,
        model=llm_config.get("model", "doubao-seed-1-8-251228"),
        temperature=llm_config.get("temperature", 0.8)
    )
    
    # æå–å“åº”æ–‡æœ¬
    if isinstance(response.content, str):
        ai_response = response.content
    else:
        ai_response = str(response.content)
    
    return RealtimeConversationOutput(ai_response=ai_response.strip())


# ============== èŠ‚ç‚¹6ï¼šè¯­éŸ³åˆæˆèŠ‚ç‚¹ ==============
def voice_synthesis_node(
    state: VoiceSynthesisInput,
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> VoiceSynthesisOutput:
    """
    title: è¯­éŸ³åˆæˆ
    desc: å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³è¾“å‡º
    integrations: è¯­éŸ³å¤§æ¨¡å‹
    """
    ctx = runtime.context
    
    # åˆå§‹åŒ–TTSå®¢æˆ·ç«¯
    tts_client = TTSClient(ctx=ctx)
    
    # æ ¹æ®å­©å­å¹´é¾„é€‰æ‹©åˆé€‚çš„è¯­éŸ³
    # 12å²ä»¥ä¸‹ä½¿ç”¨å„¿ç«¥è¯­éŸ³ï¼Œå¦åˆ™ä½¿ç”¨æ­£å¸¸è¯­éŸ³
    if state.child_age <= 12:
        voice_id = "zh_female_xueayi_saturn_bigtts"  # å„¿ç«¥è¯»ç‰©å£°éŸ³
    else:
        voice_id = "zh_female_xiaohe_uranus_bigtts"  # é»˜è®¤å¥³å£°
    
    # è¯­éŸ³åˆæˆ
    audio_url, audio_size = tts_client.synthesize(
        uid=f"child_{state.child_age}",
        text=state.text,
        speaker=voice_id,
        audio_format="mp3",
        sample_rate=24000,
        speech_rate=10,  # ç¨å¾®æ”¾æ…¢è¯­é€Ÿï¼Œé€‚åˆå­©å­
        loudness_rate=10  # æé«˜éŸ³é‡
    )
    
    return VoiceSynthesisOutput(
        audio_url=audio_url,
        audio_size=audio_size
    )


# ============== æ¡ä»¶åˆ¤æ–­å‡½æ•°ï¼šè·¯ç”±å†³ç­– ==============
def route_decision(state: RouteDecisionInput) -> str:
    """
    title: è·¯ç”±å†³ç­–
    desc: æ ¹æ®è§¦å‘ç±»å‹å’Œä½œä¸šçŠ¶æ€å†³å®šæ‰§è¡Œå“ªä¸ªåˆ†æ”¯
    """
    if state.trigger_type == "care":
        return "ä¸»åŠ¨å…³å¿ƒ"
    elif state.trigger_type == "remind":
        return "ä½œä¸šæé†’"
    elif state.trigger_type == "practice":
        return "å£è¯­ç»ƒä¹ "
    elif state.trigger_type == "conversation":
        return "å®æ—¶å¯¹è¯"
    elif state.trigger_type == "realtime_call":
        return "å®æ—¶é€šè¯"
    else:
        return "å®æ—¶å¯¹è¯"  # é»˜è®¤åˆ†æ”¯


# ============== å®æ—¶é€šè¯å¿«é€ŸèŠ‚ç‚¹ï¼ˆä½å»¶è¿Ÿä¸“ç”¨ï¼‰=============
def realtime_call_fast_node(
    state: RealtimeCallInput,  # ä½¿ç”¨RealtimeCallInput
    config: RunnableConfig,
    runtime: Runtime[Context]
) -> RealtimeCallOutput:
    """
    title: å®æ—¶é€šè¯å¿«é€ŸèŠ‚ç‚¹
    desc: æ•´åˆASR+LLM+TTSï¼Œä¸“ä¸ºå®æ—¶é€šè¯ä¼˜åŒ–ï¼Œæœ€å°åŒ–å»¶è¿Ÿ
    integrations: è¯­éŸ³å¤§æ¨¡å‹, å¤§è¯­è¨€æ¨¡å‹
    """
    from coze_coding_dev_sdk import ASRClient, LLMClient, TTSClient

    ctx = runtime.context

    # ============== æ­¥éª¤1: è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰=============
    user_text = state.user_input_text

    # å¦‚æœæœ‰éŸ³é¢‘è¾“å…¥ä¸”æ²¡æœ‰æ–‡æœ¬ï¼Œè¿›è¡Œè¯­éŸ³è¯†åˆ«
    if state.user_input_audio and not user_text:
        asr_client = ASRClient(ctx=ctx)
        try:
            text, _ = asr_client.recognize(
                uid=f"realtime_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                url=state.user_input_audio.url
            )
            user_text = text
            print(f"ğŸ¤ ASRè¯†åˆ«æˆåŠŸ: {user_text}")
        except Exception as e:
            print(f"âš ï¸ ASRè¯†åˆ«å¤±è´¥: {e}")
    
    if not user_text:
        user_text = "ä½ å¥½"
    
    # ============== æ­¥éª¤2: å¤§æ¨¡å‹ç”Ÿæˆï¼ˆLLMï¼‰=============
    llm_client = LLMClient(ctx=ctx)
    
    # è·å–å½“å‰æ—¶é—´ä¿¡æ¯
    current_time = datetime.now()
    time_of_day = "æ—©ä¸Š" if current_time.hour < 12 else "ä¸‹åˆ" if current_time.hour < 18 else "æ™šä¸Š"
    
    # ç®€åŒ–çš„æç¤ºè¯ï¼Œå‡å°‘tokenæ•°é‡
    simple_prompt = f"""ä½ æ˜¯{state.child_name}çš„å¥½æœ‹å‹ï¼Œ{state.child_age}å²ã€‚

å½“å‰æ—¶é—´ï¼š{time_of_day}
å­©å­è¯´ï¼š{user_text}

è¯·ç”¨ç®€å•ã€å‹å¥½çš„è¯­è¨€å›å¤{state.child_age}å²çš„å­©å­ã€‚ä¿æŒæ¸©æš–å’Œå…³çˆ±ã€‚

é‡è¦ï¼š
- åªè¾“å‡ºå¯¹è¯å†…å®¹
- ä¸è¦åŒ…å«åŠ¨ä½œæè¿°ï¼ˆå¦‚å¾®ç¬‘ã€ç‚¹å¤´ç­‰ï¼‰
- ä¸è¦ä½¿ç”¨è¡¨æƒ…ç¬¦å·
- å›å¤è¦ç®€çŸ­ç²¾ç‚¼ï¼ˆ50-100å­—ï¼‰
- ç›´æ¥å›ç­”é—®é¢˜æˆ–å›åº”
"""
    
    messages = [
        HumanMessage(content=simple_prompt)
    ]
    
    # ä½¿ç”¨ä½å»¶è¿Ÿé…ç½®
    response = llm_client.invoke(
        messages=messages,
        model="doubao-seed-1-8-251228",
        temperature=0.7,  # ç¨é«˜æ¸©åº¦ï¼Œæ›´è‡ªç„¶
        max_tokens=150    # é™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œå‡å°‘å»¶è¿Ÿ
    )
    
    ai_response = response.content if isinstance(response.content, str) else str(response.content)
    ai_response = ai_response.strip()
    print(f"ğŸ¤– LLMç”Ÿæˆ: {ai_response}")
    
    # ============== æ­¥éª¤3: è¯­éŸ³åˆæˆï¼ˆTTSï¼‰=============
    tts_client = TTSClient(ctx=ctx)
    
    # ä½¿ç”¨å¿«é€ŸTTSé…ç½®
    # é€‰æ‹©ä½å»¶è¿ŸéŸ³è‰²
    if state.child_age <= 12:
        voice_id = "zh_female_xueayi_saturn_bigtts"  # å„¿ç«¥éŸ³è‰²
    else:
        voice_id = "zh_female_xiaohe_uranus_bigtts"  # é»˜è®¤å¥³å£°
    
    # ä½å»¶è¿Ÿé…ç½®
    audio_url, audio_size = tts_client.synthesize(
        uid=f"realtime_{datetime.now().strftime('%Y%m%d%H%M%S')}",
        text=ai_response,
        speaker=voice_id,
        audio_format="mp3",
        sample_rate=16000,   # é™ä½é‡‡æ ·ç‡ä»¥å‡å°‘å»¶è¿Ÿ
        speech_rate=10,      # æ ‡å‡†è¯­é€Ÿ
        loudness_rate=10     # æé«˜éŸ³é‡
    )
    
    print(f"ğŸ”Š TTSåˆæˆå®Œæˆ: {audio_url}")
    
    return RealtimeCallOutput(
        ai_response=ai_response,
        ai_response_audio=audio_url,
        recognized_text=user_text
    )
